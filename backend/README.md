# InsightIQ Backend

AI-Powered Business Analytics Dashboard Backend API built with FastAPI, LangChain, and Groq.

## ğŸš€ Features

- **Natural Language to SQL**: Convert plain English queries to SQL using Groq AI
- **CSV Upload & Processing**: Upload CSV files with automatic schema generation
- **DuckDB Integration**: Fast analytical database for data processing
- **Dashboard Management**: Create and manage custom dashboards
- **Query History**: Track and retrieve past queries
- **RESTful API**: Complete API with automatic documentation

## ğŸ› ï¸ Tech Stack

- **FastAPI**: Modern, fast web framework for building APIs
- **SQLAlchemy**: SQL toolkit and ORM
- **PostgreSQL**: Primary database for metadata
- **DuckDB**: Analytical database for data processing
- **LangChain**: Framework for AI applications
- **Groq**: Fast AI inference for NL2SQL
- **Pydantic**: Data validation using Python type annotations

## ğŸ“‹ Prerequisites

- Python 3.8+
- PostgreSQL
- Groq API key

## ğŸ—ï¸ Installation

1. **Clone the repository and navigate to backend:**
   ```bash
   cd backend
   ```

2. **Create a virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables:**
   ```bash
   cp env.example .env
   # Edit .env with your configuration
   ```

5. **Configure PostgreSQL:**
   - Create a database named `insightiq`
   - Update the database URLs in `.env`

6. **Get a Groq API key:**
   - Sign up at [Groq](https://console.groq.com/)
   - Add your API key to `.env`

## ğŸ”§ Configuration

### Environment Variables

```env
# Database Configuration
DATABASE_URL=postgresql://postgres:password@localhost:5432/insightiq
DATABASE_URL_ASYNC=postgresql+asyncpg://postgres:password@localhost:5432/insightiq

# AI Configuration
GROQ_API_KEY=your_groq_api_key_here

# Security
SECRET_KEY=your-secret-key-change-in-production

# App Configuration
DEBUG=true
UPLOAD_DIR=uploads
MAX_FILE_SIZE=52428800
DUCKDB_PATH=data/insightiq.duckdb
```

## ğŸš€ Running the Application

### Development Mode
```bash
python start.py
```

### Production Mode
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The API will be available at:
- **API Base**: http://localhost:8000
- **Interactive Docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“š API Endpoints

### Queries
- `POST /api/v1/query` - Process natural language query
- `GET /api/v1/query/history` - Get query history
- `GET /api/v1/query/{query_id}` - Get specific query details

### Upload
- `POST /api/v1/upload/csv` - Upload and process CSV file
- `GET /api/v1/datasets` - List all datasets
- `GET /api/v1/datasets/{dataset_id}` - Get dataset details
- `DELETE /api/v1/datasets/{dataset_id}` - Delete dataset

### Dashboards
- `GET /api/v1/dashboards` - List all dashboards
- `GET /api/v1/dashboards/{dashboard_id}` - Get dashboard details
- `POST /api/v1/dashboards` - Create new dashboard
- `POST /api/v1/dashboards/{dashboard_id}/widgets` - Add widget to dashboard

## ğŸ” Usage Examples

### 1. Upload a CSV File
```bash
curl -X POST "http://localhost:8000/api/v1/upload/csv" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_data.csv" \
  -F "dataset_name=Sales Data" \
  -F "description=Monthly sales data for 2023"
```

### 2. Query Data with Natural Language
```bash
curl -X POST "http://localhost:8000/api/v1/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is our monthly revenue trend?",
    "dataset_id": 1
  }'
```

### 3. Get Query History
```bash
curl "http://localhost:8000/api/v1/query/history?limit=10"
```

## ğŸ—ï¸ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ query.py      # Query endpoints
â”‚   â”‚       â”œâ”€â”€ upload.py     # File upload endpoints
â”‚   â”‚       â””â”€â”€ dashboard.py  # Dashboard endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py         # Configuration settings
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ database.py       # Database connection
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ base.py          # Base model
â”‚   â”‚   â”œâ”€â”€ user.py          # User model
â”‚   â”‚   â”œâ”€â”€ dataset.py       # Dataset model
â”‚   â”‚   â”œâ”€â”€ query.py         # Query model
â”‚   â”‚   â””â”€â”€ dashboard.py     # Dashboard models
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ query.py         # Pydantic schemas
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ai_service.py    # AI/NL2SQL service
â”‚   â”‚   â””â”€â”€ data_service.py  # Data processing service
â”‚   â””â”€â”€ main.py              # FastAPI application
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ start.py                # Startup script
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Development

### Running Tests
```bash
pytest
```

### Code Formatting
```bash
black .
isort .
```

### Database Migrations
```bash
# Create migration
alembic revision --autogenerate -m "Description"

# Apply migration
alembic upgrade head
```

## ğŸš€ Deployment

### Docker
```bash
# Build image
docker build -t insightiq-backend .

# Run container
docker run -p 8000:8000 insightiq-backend
```

### Environment Variables for Production
- Set `DEBUG=false`
- Use strong `SECRET_KEY`
- Configure production database URLs
- Set up proper CORS origins

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License. 